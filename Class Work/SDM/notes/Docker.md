
<img src="https://github.com/RaviTambade/SDM/blob/main/images/Container/containers-vs-virtual-machines.jpg"/>

Of course! Let‚Äôs continue our mentorship journey: imagine you and I are sitting in the lab, you‚Äôve just grasped CI/CD, and now you‚Äôre wondering ‚Äî ‚ÄúHow do I make sure my app runs exactly the same on my laptop, on Jenkins, on AWS, or anywhere?‚Äù That‚Äôs where **containerization with Docker** comes into play. Let‚Äôs dive in ‚Äî mentor style:

---

## üë®‚Äçüè´ Mentor Storytelling: **Containerization & Docker**

---

"Alright team, picture this: you‚Äôve spent days perfecting your app on your laptop, it works flawlessly. You push it to the cloud ‚Äî and suddenly, it breaks. Different OS, different libraries, missing dependencies‚Ä¶ the classic ‚Äúit works on my machine‚Äù nightmare.

This was *the* biggest pain point for developers for decades. But then came containerization ‚Äî like a magic box you put your app in, so it runs the same *everywhere*."

---

### üì¶ **What is Containerization?**

I want you to think of your application like a toy. If you ship it without packaging, it might break on the way ‚Äî that‚Äôs deploying without containers.

Containers are like sturdy boxes. You put your toy (app) inside with all the screws (dependencies), batteries (runtime), and instruction manual (config) it needs. Now, whether it‚Äôs opened in Mumbai, New York, or Tokyo ‚Äî the toy works exactly as you intended.

In software, containerization means **packaging your application code with its runtime, libraries, and environment into a single lightweight unit** called a *container*. The result? Consistent behavior across dev, test, staging, and production.

---

### üê≥ **What is Docker?**

When we talk about containers today, we‚Äôre mostly talking about **Docker**. Docker revolutionized how we build, share, and run containers. It made them fast, easy to use, and portable.

Before Docker, setting up isolated environments was slow and heavy (think: virtual machines). Docker made containers lightweight and near-instantaneous.

---

### üîé **Why Containers Matter**

I always tell my mentees: Containers give you three superpowers:

‚úÖ **Consistency** ‚Äì Your app runs the same everywhere.
‚úÖ **Portability** ‚Äì Ship your container image and run it on any cloud, VM, or even a Raspberry Pi.
‚úÖ **Efficiency** ‚Äì Unlike VMs, containers share the OS kernel, so they start up in seconds and use fewer resources.

---

### üõ†Ô∏è **How Does Docker Work?**

Let‚Äôs unpack this with a story:

1Ô∏è‚É£ You write a **Dockerfile** ‚Äî a recipe that tells Docker how to build your container.
2Ô∏è‚É£ You run `docker build` ‚Äî Docker reads your recipe and creates a **Docker image** (the blueprint of your app).
3Ô∏è‚É£ You run `docker run` ‚Äî Docker spins up a **container** based on your image (like creating a new car from your blueprint).
4Ô∏è‚É£ You ship this image to any machine ‚Äî it will create the same container.

---

### üìù **Let‚Äôs Create Your First Dockerfile Together**

Imagine you have a Node.js app with `app.js`. Here‚Äôs a simple Dockerfile:

```dockerfile
# 1) Start from an official Node.js image
FROM node:18

# 2) Set working directory
WORKDIR /usr/src/app

# 3) Copy package.json and install dependencies
COPY package*.json ./
RUN npm install

# 4) Copy the rest of your app code
COPY . .

# 5) Expose the port your app runs on
EXPOSE 3000

# 6) Command to run your app
CMD ["node", "app.js"]
```

This Dockerfile tells Docker:

* Which OS and Node version to use (`node:18`).
* Where your code lives (`/usr/src/app`).
* How to install dependencies and start your app.

---

### ‚öôÔ∏è **Building & Running Your Container**

From the folder with your Dockerfile, let‚Äôs build your image:

```bash
docker build -t my-node-app .
```

Now run your container:

```bash
docker run -d -p 3000:3000 my-node-app
```

* `-d` runs it in the background.
* `-p 3000:3000` maps your host‚Äôs port 3000 to the container‚Äôs port 3000.

Now go to `http://localhost:3000` ‚Äî your app is live in a container!

---

### üèóÔ∏è **Docker Images vs Containers**

* **Image** ‚Üí Blueprint of your app (immutable).
* **Container** ‚Üí Running instance of that image (like an object from a class).

---

### üåç **Docker Hub: Sharing Made Easy**

You don‚Äôt have to keep your images local. You can push them to **Docker Hub**, a cloud registry, so you or your team can pull and run them anywhere:

```bash
docker login
docker tag my-node-app yourdockerhubusername/my-node-app
docker push yourdockerhubusername/my-node-app
```

Then on any machine:

```bash
docker pull yourdockerhubusername/my-node-app
docker run -d -p 3000:3000 yourdockerhubusername/my-node-app
```

Now your app is deployable worldwide ‚Äî in minutes.

---

### üóÇÔ∏è **Containers vs Virtual Machines: A Quick Reality Check**

|                  | Containers    | Virtual Machines     |
| ---------------- | ------------- | -------------------- |
| **Startup Time** | Seconds       | Minutes              |
| **Resources**    | Lightweight   | Heavyweight          |
| **Isolation**    | Process-level | Full OS-level        |
| **Use Case**     | Microservices | Legacy apps, full OS |

I always remind students: **containers don‚Äôt replace VMs**, they complement them. For microservices or cloud-native apps, containers are the way to go. For monoliths or apps needing a full OS, VMs still have their place.

---

### üö¢ **Docker Compose: Orchestrating Multiple Containers**

What if your app uses a database too? Docker Compose lets you define multiple containers in one YAML file.

Example `docker-compose.yml`:

```yaml
version: '3'
services:
  app:
    build: .
    ports:
      - "3000:3000"
    depends_on:
      - db
  db:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: mydb
```

Run everything with:

```bash
docker-compose up -d
```

Your app and MySQL spin up together ‚Äî connected and isolated.

---

### üèÅ **Why Containerization Will Make You a Better Developer**

By embracing Docker, you:
‚úÖ Eliminate ‚Äúworks on my machine‚Äù problems.
‚úÖ Ship consistent, reliable builds.
‚úÖ Enable rapid scaling in production.
‚úÖ Future-proof your DevOps skills ‚Äî containerization is a must-have in modern cloud environments.

---

üë®‚Äçüè´ *Remember*: Don‚Äôt fear containers. Start simple, build confidence, and soon you‚Äôll wonder how you ever worked without them.

---






# What is a Container?

A standardized unit of software.
Package Software into Standardized Units for Development, Shipment and Deployment

A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.

# Comparing Containers and Virtual Machines

Containers and virtual machines have similar resource isolation and allocation benefits, but function differently because containers virtualize the operating system instead of hardware. Containers are more portable and efficient.


The Old Way to deploy applications was to install the applications on a host using the operating-system package manager. This had the disadvantage of entangling the applications‚Äô executables, configuration, libraries, and lifecycles with each other and with the host OS. One could build immutable virtual-machine images in order to achieve predictable rollouts and rollbacks, but VMs are heavyweight and non-portable.

The New Way is to deploy containers based on operating-system-level virtualization rather than hardware virtualization. These containers are isolated from each other and from the host: they have their own filesystems, they can‚Äôt see each others‚Äô processes, and their computational resource usage can be bounded. They are easier to build than VMs, and because they are decoupled from the underlying infrastructure and from the host filesystem, they are portable across clouds and OS distributions.

Because containers are small and fast, one application can be packed in each container image. This one-to-one application-to-image relationship unlocks the full benefits of containers. With containers, immutable container images can be created at build/release time rather than deployment time, since each application doesn‚Äôt need to be composed with the rest of the application stack, nor married to the production infrastructure environment. Generating container images at build/release time enables a consistent environment to be carried from development into production. Similarly, containers are vastly more transparent than VMs, which facilitates monitoring and management. This is especially true when the containers‚Äô process lifecycles are managed by the infrastructure rather than hidden by a process supervisor inside the container. Finally, with a single application per container, managing the containers becomes tantamount to managing deployment of the application.


## VIRTUAL MACHINES
Virtual machines (VMs) are an abstraction of physical hardware turning one server into many servers. The hypervisor allows multiple VMs to run on a single machine. Each VM includes a full copy of an operating system, the application, necessary binaries and libraries - taking up tens of GBs. VMs can also be slow to boot.


## CONTAINERS

Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can run on the same machine and share the OS kernel with other containers, each running as isolated processes in user space. Containers take up less space than VMs (container images are typically tens of MBs in size), can handle more applications and require fewer VMs and Operating systems.


## Containerization

Containerization is a lightweight alternative to full machine virtualization that involves encapsulating an application in a container with its own operating environment. This provides many of the benefits of loading an application onto a virtual machine, as the application can be run on any suitable physical machine without any worries about dependencies.

Containerization has gained recent prominence with the open-source Docker. Docker containers are designed to run on everything from physical computers to virtual machines, bare-metal servers, OpenStack cloud clusters, public instances and more.


## Docker

The widespread adoption of containers has changed the orthodoxy of IT‚Äîadministrators do not need to run multiple virtual machines to create a barrier between applications or users, nor are programmers beholden to languages such as Java, which have their own strategy for process isolation and lifecycle management.

Leading the way in this change is Docker, an open-source utility that automates the deployment and management of programs inside software containers. Docker is also the center of the container ecosystem, with other solutions such as Kubernetes available for deploying containers through Docker in a computing cluster.


Docker is an open-source tool that automates the deployment of an application inside a software container. 

Back in the day, transportation companies faced the following challenges:

How to transport different (incompatible) types of goods side by side (like food and chemicals, or glass and bricks).
How to handle packages of various sizes using the same vehicle.
After the introduction of containers, bricks could be put over glass, and chemicals could be stored next to food. Cargo of various sizes can be put inside a standardized container and loaded/unloaded by the same vehicle.

When you develop an application, you need to provide your code along with all possible dependencies like libraries, the web server, databases, etc. You may end up in a situation when the application is working on your computer, but won‚Äôt even start on the staging server, or the dev or QA‚Äôs machine.

This challenge can be addressed by isolating the app to make it independent of the system.

## How does this differ from virtualization?

Traditionally, virtual machines were used to avoid this unexpected behavior. The main problem with VM is that an ‚Äúextra OS‚Äù on top of the host operating system adds gigabytes of space to the project. Most of the time your server will host several VMs that will take up even more space. And by the way, at the moment, most cloud-based server providers will charge you for that extra space. Another significant drawback of VM is a slow boot.

Docker eliminates all the above by simply sharing the OS kernel across all the containers running as separate processes of the host OS.


## Why do we need Docker?

The short list of benefits includes:

- Faster development process
- Handy application encapsulation
- The same behaviour on local machine / dev / staging / production servers
- Easy and clear monitoring
- Easy to scale


### Faster development process

There is no need to install 3rd-party apps like PostgreSQL, Redis, Elasticsearch on the system ‚Äì you can run it in containers. Docker also gives you the ability to run different versions of same application simultaneously. For example, say you need to do some manual data migration from an older version of Postgres to a newer version. You can have such a situation in microservice architecture when you want to create a new microservice with a new version of the 3rd-party software.

It could be quite complex to keep two different versions of the same app on one host OS. In this case, Docker containers could be a perfect solution ‚Äì you receive isolated environments for your applications and 3rd-parties.

### Handy application encapsulation

You can deliver your application in one piece. Most programming languages, frameworks and all operating systems have their own packaging managers. And even if your application can be packed with its native package manager, it could be hard to create a port for another system.

Docker gives you a unified image format to distribute you applications across different host systems and cloud services. You can deliver your application in one piece with all the required dependencies (included in an image) ready to run.

Same behaviour on local machine / dev / staging / production servers
Docker can‚Äôt guarantee 100% dev / staging / production parity, because there is always the human factor. But it reduces to almost zero the probability of error caused by different versions of operating systems, system-dependencies, etc.

With right approach to building Docker images, your application will use the same base image with the same OS version and the required dependencies.

### Easy and clear monitoring

Out of the box, you have a unified way to read log files from all running containers. You don‚Äôt need to remember all the specific paths where your app and its dependencies store log files and write custom hooks to handle this.
You can integrate an external logging driver and monitor your app log files in one place.

### Easy to scale

A correctly wrapped application will cover most of the Twelve Factors. By design, Docker forces you follow its core principles, such as configuration over environment variables, communication over TCP/UDP ports, etc. And if you‚Äôve done your application right, it will be ready for scaling not only in Docker.

### Supported platforms

Docker‚Äôs native platform is Linux, as it‚Äôs based on features provided by the Linux kernel. However, you can still run it on macOS and Windows. 
The only difference is that on macOS and Windows, Docker is encapsulated into a tiny virtual machine. At the moment, Docker for macOS and Windows has reached a significant level of usability and feels more like a native app.

## Terminology

#### Container 
‚Äì a running instance that encapsulates required software. Containers are always created from images. A container can expose ports and volumes to interact with other containers or/and the outer world. Containers can be easily killed / removed and re-created again in a very short time. Containers don‚Äôt keep state.

#### Image 
‚Äì the basic element for every container. When you create an image, every step is cached and can be reused (Copy On Write model). Depending on the image, it can take some time to build. Containers, on the other hand, can be started from images right away.

#### Port 
‚Äì a TCP/UDP port in its original meaning. To keep things simple, let‚Äôs assume that ports can be exposed to the outer world (accessible from the host OS) or connected to other containers ‚Äì i.e., accessible only from those containers and invisible to the outer world.

#### Volume
‚Äì can be described as a shared folder. Volumes are initialized when a container is created. Volumes are designed to persist data, independent of the container‚Äôs lifecycle.

#### Registry 
‚Äì the server that stores Docker images. It can be compared to Github ‚Äì you can pull an image from the registry to deploy it locally, and push locally built images to the registry.

## Docker Hub 
‚Äì a registry with web interface provided by Docker Inc. It stores a lot of Docker images with different software. Docker Hub is a source of the ‚Äúofficial‚Äù Docker images made by the Docker team or in cooperation with the original software manufacturer (it doesn‚Äôt necessary mean that these ‚Äúoriginal‚Äù images are from official software manufacturers). Official images list their potential vulnerabilities. This information is available to any logged-in user. There are both free and paid accounts available. You can have one private image per account and an infinite amount of public images for free. Docker Store ‚Äì a service very similar to Docker Hub. It‚Äôs a marketplace with ratings, reviews, etc. My personal opinion is that it‚Äôs marketing stuff. I‚Äôm totally happy with Docker Hub.


Docker can be used on all types of projects, regardless of size and complexity.

In the beginning, you can start with compose and Swarm. When the project grows, you can migrate to cloud services like Amazon Container Services or Kubernetes.

Like standard containers used in cargo transportation, wrapping your code in Docker containers will help you build faster and more efficient CI/CD processes.' This is not just another technological trend promoted by a bunch of geeks ‚Äì it‚Äôs a new paradigm that is already being used in the architecture of large companies like PayPal, Visa, Swisscom, General Electric, Splink, etc.


## Kubernetes

Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.


Kubernetes has a number of features. It can be thought of as:

- a container platform
- a microservices platform
- a portable cloud platform and a lot more.

Kubernetes provides a container-centric management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads. This provides much of the simplicity of Platform as a Service (PaaS) with the flexibility of Infrastructure as a Service (IaaS), and enables portability across infrastructure providers.


Suppose that you've started a new job as a software developer at the world's most popular pizza joint - Contoso Pizza. Business is booming and the Contoso Pizza's website that indicates whether pizzas are in stock or not has recently been refactored into microservices hosted in Docker containers.

In a microservice-based development approach, each microservice owns its model and data so that it will be autonomous from a development and deployment point of view from other microservices. Hosting microservices inside of a container is a common way to achieve that. These kinds of systems are complex to scale out and manage. You need to consider the process of organizing, adding, removing, and updating many containers. This process is referred to as container management.

For example, you may find during specific times of the day you need to scale up the number of container instances that handle caching. Or you may have an update to the container instance that checks pizza inventory.

To help with container management tasks, you can use a container orchestrator. Kubernetes is one such orchestrator. It is an extensible open-source platform for managing and orchestrating containerized workloads.

This module will teach you about Kubernetes and the problems it solves and how to deploy a .NET web API and web app into a Kubernetes cluster.


## What are orchestrators?

The decoupled design of microservices combined with the atomicity of containers make it possible to scale out apps, and respond to increased demand by deploying more container instances, and to scale back if demand is decreasing. In complex solutions the process of deploying, updating, monitoring, and removing containers introduces challenges.



## Container management

Container management is the process of organizing, adding, removing, or updating a significant number of containers.

The Contoso Pizza Company website consists of multiple microservices responsible for tasks like caching, data processing, and a shopping cart. Each of these services is hosted in a container and can be deployed, updated, and scaled independently from one another.


If you increase the number of a shopping cart container instances and need to deploy a new version, you'll have to update every single instance of that container.

Container management helps with these tasks.


## Container orchestration


A container orchestrator is a system that automatically deploys and manages containerized apps. For example, the orchestrator can dynamically respond to changes in the environment to increase or decrease the deployed instances of the managed app. Or, it can ensure all deployed container instances get updated if a new version of a service is released.



## Kubernetes Benefits

The benefits of using Kubernetes are based on the abstraction of tasks.


These tasks include:

- Self-healing of containers. An example would be restarting containers that fail or replacing containers.
- Scaling deployed container count up or down dynamically, based on demand.
- Automating rolling updates and rollbacks of containers.
- Managing storage.
- Managing network traffic.
- Storing and managing sensitive information, such as usernames and passwords.

Because Kuberentes is a tool to orchestrate containerized workloads, and you can deploy .NET, java, NodeJs, Python  Microservices into containers, you can use Kubernetes to orchestrate your .NET , Java, NodeJs, Python microservices. 

### Complete Picture of CD/CI Piplines in Software Engg.
<img src="https://github.com/RaviTambade/SDM/blob/main/images/Container/CD_CI.jpg"/>
